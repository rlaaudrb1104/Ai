{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlaaudrb1104/Ai/blob/PJH/graphcodebert_CWE_%EB%8B%A4%EC%A4%91_%EB%AA%A8%EB%8D%B8_%ED%99%95%EB%A5%A0_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV5hUdJXMK4R",
        "outputId": "7e30d5d4-88f4-4680-ad71-f9f828b82522"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABaFENrmmTsf",
        "outputId": "97ad529f-1739-4d08-eaf1-3abcc50fbae9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import re\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "models = {\n",
        "    \"cwe-119\": AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/models/CWE_models/CWE-119_vul\"),\n",
        "    \"cwe-20\": AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/models/CWE_models/CWE-20_vul\"),\n",
        "    \"cwe-125\": AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/models/CWE_models/CWE-125_vul\"),\n",
        "    \"cwe-787\": AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/models/CWE_models/CWE-787_vul\")\n",
        "}\n",
        "\n",
        "def preprocess2(code_text):\n",
        "    # 주석 제거\n",
        "    code_text = re.sub(r'//.*?\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'/\\*.*?\\*/', '', code_text, flags=re.MULTILINE)\n",
        "    code_text = re.sub(r'^#define\\s+<.*?>\\s*\\n', '', code_text, flags=re.MULTILINE)\n",
        "    code_text = re.sub(r'#include .*?\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#include \"*.*?\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#define .*?\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#undef\\s+\\w+', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#if\\s+\\w+', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#else\\s+\\w+', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#elif\\s+\\w+', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'#endif', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'[\\t ]+', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'\\n\\s*\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'\\n', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'return*.*?;', '', code_text, flags=re.DOTALL)\n",
        "    code_text = re.sub(r'return;', '', code_text, flags=re.DOTALL)\n",
        "\n",
        "    # 코드 내의 공백 및 줄 바꿈 제거\n",
        "    code_text = re.sub(r'\\s+', ' ', code_text)\n",
        "\n",
        "    return code_text\n",
        "\n",
        "def predict_vulnerability(input_code, models, tokenizer):\n",
        "    # 입력 코드 전처리\n",
        "    preprocessed_text = preprocess2(input_code)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        # 입력 토크나이징\n",
        "        inputs = tokenizer(preprocessed_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        # 모델 예측\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.sigmoid(logits).squeeze()  # 시그모이드 함수로 확률 계산\n",
        "\n",
        "            # 취약점 발생 확률 (레이블 1) 예측\n",
        "            vulnerability_probability = probs[1].item()  # 레이블 1의 확률\n",
        "\n",
        "            # 결과 저장\n",
        "            results[model_name] = {\n",
        "                \"vulnerability_probability\": vulnerability_probability\n",
        "            }\n",
        "\n",
        "    # 가장 높은 확률의 모델 선택\n",
        "    best_model = max(results, key=lambda k: results[k][\"vulnerability_probability\"])\n",
        "\n",
        "    return results, best_model\n",
        "\n",
        "# 입력 코드 예시\n",
        "input_code = \"\"\"\n",
        "void host_lookup(char *user_supplied_addr){\n",
        "struct hostent *hp;\n",
        "in_addr_t *addr;\n",
        "char hostname[64];\n",
        "in_addr_t inet_addr(const char *cp);\n",
        "\n",
        "/*routine that ensures user_supplied_addr is in the right format for conversion */\n",
        "\n",
        "validate_addr_form(user_supplied_addr);\n",
        "addr = inet_addr(user_supplied_addr);\n",
        "hp = gethostbyaddr( addr, sizeof(struct in_addr), AF_INET);\n",
        "strcpy(hostname, hp->h_name);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 취약점 예측 실행\n",
        "predictions, best_model = predict_vulnerability(input_code, models, tokenizer)\n",
        "\n",
        "# 결과 출력\n",
        "for model_name, result in predictions.items():\n",
        "    vulnerability_probability = result[\"vulnerability_probability\"]\n",
        "    vulnerability_status = \"취약점 존재\" if vulnerability_probability >= 0.5 else \"취약점 없음\"\n",
        "    print(f\"{model_name}: {vulnerability_status}, Probability: {vulnerability_probability:.4f}\")\n",
        "\n",
        "if best_model:\n",
        "    print(f\"\\n가장 높은 확률을 가진 모델: {best_model}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a43xUvNulotj",
        "outputId": "264a9b79-394f-4f3f-f9b0-64e7c124ed82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwe-119: 취약점 없음, Probability: 0.1700\n",
            "cwe-20: 취약점 없음, Probability: 0.1987\n",
            "cwe-125: 취약점 없음, Probability: 0.1622\n",
            "cwe-787: 취약점 없음, Probability: 0.0952\n",
            "\n",
            "가장 높은 확률을 가진 모델: cwe-20\n"
          ]
        }
      ]
    }
  ]
}