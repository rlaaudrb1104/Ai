{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlaaudrb1104/Ai/blob/WOOK/GraphCodeBERT_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install easydict"
      ],
      "metadata": {
        "id": "onNmXyNHZ7CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2I4SFwJZ7MJ",
        "outputId": "a8076fe7-20d7-4815-ce89-f89511cce606"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pickle\n",
        "from easydict import EasyDict as edict\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "WXDXFSr0uAr6"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 로깅 설정\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "qPC6_DPcuCPK"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인자 설정 클래스\n",
        "args = edict({\n",
        "    'train_data_file': '/content/drive/MyDrive/final_train2.csv',\n",
        "    'eval_data_file': '/content/drive/MyDrive/final_val.csv',\n",
        "    'test_data_file': '/content/drive/MyDrive/Colab Notebooks/cwe file/final_train.csv',\n",
        "    'output_dir': '/content/drive/My Drive/output',\n",
        "    'model_name_or_path': 'microsoft/graphcodebert-base',\n",
        "    'tokenizer_name': 'microsoft/graphcodebert-base',\n",
        "    'block_size': 512,\n",
        "    'use_logit_adjustment': False,\n",
        "    'tau': 1.2,\n",
        "    'model_type': 'bert',\n",
        "    'code_length': 256,\n",
        "    'do_train': True,\n",
        "    'do_eval': True,\n",
        "    'do_test': True,\n",
        "    'evaluate_during_training': False,\n",
        "    'do_local_explanation': False,\n",
        "    'reasoning_method': None,\n",
        "    'train_batch_size': 4,\n",
        "    'eval_batch_size': 4,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'learning_rate': 5e-5,\n",
        "    'weight_decay': 0.0,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'max_steps': -1,\n",
        "    'warmup_steps': 0,\n",
        "    'seed': 42,\n",
        "    'epochs': 1,\n",
        "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    'n_gpu': torch.cuda.device_count(),\n",
        "    'cwe_label_map': {\n",
        "        \"CWE-20\": 2,\n",
        "        \"CWE-119\": 1,\n",
        "        \"CWE-78\": 3,\n",
        "        \"CWE-122\": 4,\n",
        "        \"CWE-121\": 5,\n",
        "        \"CWE-415\": 6,\n",
        "        \"CWE-399\": 7,\n",
        "        \"CWE-190\": 8,\n",
        "        \"CWE-125\": 9,\n",
        "        \"CWE-416\": 10\n",
        "    }\n",
        "})"
      ],
      "metadata": {
        "id": "Vx0Rjx1hc4Uq"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 설정\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)"
      ],
      "metadata": {
        "id": "g_VsNoT1uH1J"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InputFeatures 클래스 정의\n",
        "class InputFeatures:\n",
        "    \"\"\"데이터의 한 세트의 특성을 정의합니다.\"\"\"\n",
        "    def __init__(self, input_ids, attention_mask, cwe_type_label):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.cwe_type_label = cwe_type_label\n"
      ],
      "metadata": {
        "id": "zEt6V2DbsqWi"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 변환 함수\n",
        "def convert_examples_to_features(func, cwe_type_label, tokenizer, max_length):\n",
        "    \"\"\"코드 스니펫을 모델 입력에 적합한 특성으로 변환합니다.\"\"\"\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text=func,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "    return InputFeatures(\n",
        "        input_ids=encoding['input_ids'].flatten(),\n",
        "        attention_mask=encoding['attention_mask'].flatten(),\n",
        "        cwe_type_label=cwe_type_label\n",
        "    )"
      ],
      "metadata": {
        "id": "2TviToGqm_3w"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cwe_label_map = {\n",
        "    \"CWE-20\": 2,\n",
        "    \"CWE-119\": 1,\n",
        "    \"CWE-78\": 3,\n",
        "    \"CWE-122\": 4,\n",
        "    \"CWE-121\": 5,\n",
        "    \"CWE-415\": 6,\n",
        "    \"CWE-399\": 7,\n",
        "    \"CWE-190\": 8,\n",
        "    \"CWE-125\": 9,\n",
        "    \"CWE-416\": 10\n",
        "    # 여기에 더 많은 CWE ID와 인덱스 매핑을 추가할 수 있습니다.\n",
        "}"
      ],
      "metadata": {
        "id": "gMV2azhUpSGO"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextDataset 클래스 정의\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, args, cwe_label_map, file_type=\"train\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cwe_label_map = args.cwe_label_map\n",
        "        file_path = getattr(args, f\"{file_type}_data_file\")\n",
        "        self.examples = []\n",
        "        df = pd.read_csv(file_path)\n",
        "        funcs = df[\"CODE\"].tolist()\n",
        "        cwe_type_labels = df[\"CWE ID\"].tolist()\n",
        "\n",
        "        for i in tqdm(range(len(funcs))):\n",
        "            cwe_type_label = self.cwe_label_map.get(cwe_type_labels[i], 0)\n",
        "            features = convert_examples_to_features(funcs[i], cwe_type_label, tokenizer, args.block_size)\n",
        "            self.examples.append(features)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.examples[idx].input_ids),\n",
        "            'attention_mask': torch.tensor(self.examples[idx].attention_mask),\n",
        "            'labels': torch.tensor(self.examples[idx].cwe_type_label)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6qFg0f4XuTCq"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋과 데이터 로더 생성\n",
        "train_dataset = TextDataset(tokenizer, args, cwe_label_map, file_type='train')\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uvoTMU8uYfy",
        "outputId": "daa8721a-f2da-44de-caa4-6867c4384826"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-109-533f6d5cf39f>:8: DtypeWarning: Columns (107) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "  0%|          | 0/26228 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 26228/26228 [01:27<00:00, 299.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_adjustment(tau, args, cwe_label_map):\n",
        "    \"\"\"클래스 빈도에 따라 로짓 조정 값을 계산합니다.\n",
        "\n",
        "    Args:\n",
        "        tau (float): 조정 계수, 클래스 빈도의 지수화에 사용됩니다.\n",
        "        args (Args class): 설정 정보를 담고 있는 클래스 인스턴스.\n",
        "        cwe_label_map (dict): 각 CWE ID에 대한 [index, one_hot, frequency] 정보를 담은 사전.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 조정된 로짓 값들이 담긴 텐서.\n",
        "    \"\"\"\n",
        "    # 빈도 수집: 사전 순서대로 빈도를 배열에 저장\n",
        "    freq = [v[2] for k, v in sorted(cwe_label_map.items(), key=lambda item: item[1][0])]\n",
        "\n",
        "    # 레이블 빈도를 PyTorch 텐서로 변환하고 정규화\n",
        "    label_freq_tensor = torch.tensor(freq, dtype=torch.float32, device=args.device)\n",
        "    label_freq_tensor /= label_freq_tensor.sum()\n",
        "\n",
        "    # 로짓 조정 계산\n",
        "    adjustments = torch.log(torch.pow(label_freq_tensor, tau) + 1e-12)\n",
        "\n",
        "    return adjustments\n"
      ],
      "metadata": {
        "id": "8LLkkGJ8uZHD"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_adjustment(tau, args, cwe_label_map):\n",
        "    \"\"\"클래스 빈도에 따라 로짓 조정 값을 계산합니다.\n",
        "\n",
        "    Args:\n",
        "        tau (float): 조정 계수, 클래스 빈도의 지수화에 사용됩니다.\n",
        "        args (Args class): 설정 정보를 담고 있는 클래스 인스턴스.\n",
        "        cwe_label_map (dict): 각 CWE ID에 대한 [index, one_hot, frequency] 정보를 담은 사전.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 조정된 로짓 값들이 담긴 텐서.\n",
        "    \"\"\"\n",
        "    # 빈도 수집: 사전 순서대로 빈도를 배열에 저장\n",
        "    freq = [v[2] for k, v in sorted(cwe_label_map.items(), key=lambda item: item[1][0])]\n",
        "\n",
        "    # 레이블 빈도를 PyTorch 텐서로 변환하고 정규화\n",
        "    label_freq_tensor = torch.tensor(freq, dtype=torch.float32, device=args.device)\n",
        "    label_freq_tensor /= label_freq_tensor.sum()\n",
        "\n",
        "    # 로짓 조정 계산\n",
        "    adjustments = torch.log(torch.pow(label_freq_tensor, tau) + 1e-12)\n",
        "\n",
        "    return adjustments"
      ],
      "metadata": {
        "id": "acpqOVO4DxGh"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def set_seed(args):\n",
        "    \"\"\"\n",
        "    모든 난수 생성기의 시드를 설정하여 실험의 재현성을 보장합니다.\n",
        "\n",
        "    Args:\n",
        "    - args (Args class): 시드와 GPU 사용 여부를 포함하는 설정 클래스 인스턴스.\n",
        "\n",
        "    이 함수는 Python의 내장 난수 생성기, NumPy, 그리고 PyTorch에 대해 동일한 시드를 설정합니다.\n",
        "    또한, 여러 GPU를 사용하는 경우 각 디바이스의 CUDA 난수 생성기에 대해서도 시드를 설정합니다.\n",
        "    \"\"\"\n",
        "    random.seed(args.seed)  # Python 내장 난수 생성기의 시드 설정\n",
        "    np.random.seed(args.seed)  # NumPy 난수 생성기의 시드 설정\n",
        "    torch.manual_seed(args.seed)  # PyTorch 난수 생성기의 시드 설정\n",
        "\n",
        "    # CUDA 난수 생성기의 시드 설정 (GPU 사용 시)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "# 예시로 Args 클래스를 정의하고 seed와 n_gpu를 설정합니다.\n",
        "class Args:\n",
        "    seed = 42\n",
        "    n_gpu = 1  # 이 값을 Colab에서 사용할 GPU 수에 맞춰 설정하세요.\n",
        "\n",
        "args = Args()\n",
        "set_seed(args)  # 시드 설정 함수 호출\n"
      ],
      "metadata": {
        "id": "RM9vMADuEG-p"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(args, train_dataset, model, tokenizer, eval_dataset):\n",
        "    \"\"\" 모델을 훈련합니다. \"\"\"\n",
        "    cwe_label_map = args.cwe_label_map\n",
        "    # 데이터 로더 구성\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n",
        "\n",
        "    # 로짓 조정 사용 여부 확인\n",
        "    if args.use_logit_adjustment:\n",
        "        logit_adjustment = compute_adjustment(args.tau, args, cwe_label_map)\n",
        "    else:\n",
        "        logit_adjustment = None\n",
        "\n",
        "    # 최적화 도구 및 스케줄러 설정\n",
        "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=args.max_steps)\n",
        "\n",
        "    model.to(args.device)\n",
        "    model.train()\n",
        "\n",
        "    # 훈련 루프\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "            batch = {k: v.to(args.device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # multi-gpu 지원\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        # 에폭 종료 후 평가\n",
        "        if eval_dataset is not None:\n",
        "            eval_loss = evaluate(args, model, tokenizer, eval_dataset)\n",
        "            print(f\"Epoch {epoch + 1} Evaluation Loss: {eval_loss:.4f}\")\n",
        "\n",
        "        # 로스 감소 시 체크포인트 저장\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            save_checkpoint(model, args.output_dir, 'best_model.pt')\n",
        "\n",
        "def save_checkpoint(model, save_path, filename):\n",
        "    \"\"\" 모델 상태를 파일로 저장합니다. \"\"\"\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    torch.save(model.state_dict(), os.path.join(save_path, filename))\n"
      ],
      "metadata": {
        "id": "Cit80wEBD6mR"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n",
        "    # DataLoader 생성\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, num_workers=0)\n",
        "\n",
        "    # Multi-GPU 설정\n",
        "    if args.n_gpu > 1 and not eval_when_training:\n",
        "        original_model = model\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # 모델 평가 모드 설정\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # 평가 진행\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"Batch size = %d\", args.eval_batch_size)\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(args.device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            total_examples += batch['input_ids'].size(0)\n",
        "\n",
        "    # 평균 손실 계산\n",
        "    avg_loss = total_loss / total_examples\n",
        "\n",
        "    # Multi-GPU 상태를 원래대로 복원\n",
        "    if args.n_gpu > 1 and not eval_when_training:\n",
        "        model = original_model\n",
        "\n",
        "    # 결과 로깅\n",
        "    logger.info(\"***** Eval results *****\")\n",
        "    logger.info(\"Total loss = %.4f\", avg_loss)\n",
        "\n",
        "    # 결과 반환\n",
        "    result = {\n",
        "        \"eval_total_loss\": avg_loss\n",
        "    }\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "wiW2h08tEb8B"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def test(args, model, tokenizer, test_dataset, best_threshold=0.5):\n",
        "    # DataLoader 구성\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n",
        "\n",
        "    # 다중 GPU 설정\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # 모델 평가 모드 설정\n",
        "    model.eval()\n",
        "    logger.info(\"***** Running Test *****\")\n",
        "    logger.info(\"Num examples = %d\", len(test_dataset))\n",
        "    logger.info(\"Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    # 예측 및 실제 레이블 저장용 리스트\n",
        "    cwe_type_preds = []\n",
        "    cwe_type_trues = []\n",
        "\n",
        "    # 테스트 데이터에 대한 평가\n",
        "    for batch in test_dataloader:\n",
        "        batch = {k: v.to(args.device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            cwe_type_preds.extend(predictions.cpu().numpy())\n",
        "            cwe_type_trues.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    # 정확도 계산\n",
        "    cwe_type_acc = accuracy_score(cwe_type_trues, cwe_type_preds)\n",
        "    result_cwe = {\"CWE Type Accuracy\": cwe_type_acc}\n",
        "\n",
        "    # 결과 로깅\n",
        "    logger.info(\"***** CWE Type Classification Test Results *****\")\n",
        "    for key, value in result_cwe.items():\n",
        "        logger.info(f\"{key} = {value:.4f}\")\n",
        "\n",
        "    return result_cwe\n"
      ],
      "metadata": {
        "id": "SxigSWdzE_gt"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    args = edict({\n",
        "        'train_data_file': '/content/drive/MyDrive/final_train2.csv',\n",
        "        'eval_data_file': '/content/drive/MyDrive/final_val.csv',\n",
        "        'test_data_file': '/content/drive/MyDrive/Colab Notebooks/cwe file/final_train.csv',\n",
        "        'output_dir': '/content/drive/My Drive/output',\n",
        "        'model_name_or_path': 'microsoft/graphcodebert-base',\n",
        "        'tokenizer_name': 'microsoft/graphcodebert-base',\n",
        "        'block_size': 512,\n",
        "        'use_logit_adjustment': False,\n",
        "        'tau': 1.2,\n",
        "        'model_type': 'bert',\n",
        "        'code_length': 256,\n",
        "        'do_train': True,\n",
        "        'do_eval': True,\n",
        "        'do_test': True,\n",
        "        'evaluate_during_training': False,\n",
        "        'do_local_explanation': False,\n",
        "        'reasoning_method': None,\n",
        "        'train_batch_size': 8,\n",
        "        'eval_batch_size': 8,\n",
        "        'gradient_accumulation_steps': 1,\n",
        "        'learning_rate': 2e-5,\n",
        "        'weight_decay': 0.0,\n",
        "        'adam_epsilon': 1e-8,\n",
        "        'max_grad_norm': 1.0,\n",
        "        'max_steps': -1,\n",
        "        'warmup_steps': 0,\n",
        "        'seed': 42,\n",
        "        'epochs': 1,\n",
        "        'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        'n_gpu': torch.cuda.device_count(),\n",
        "        'cwe_label_map': {\n",
        "        \"CWE-20\": 2,\n",
        "        \"CWE-119\": 1,\n",
        "        \"CWE-78\": 3,\n",
        "        \"CWE-122\": 4,\n",
        "        \"CWE-121\": 5,\n",
        "        \"CWE-415\": 6,\n",
        "        \"CWE-399\": 7,\n",
        "        \"CWE-190\": 8,\n",
        "        \"CWE-125\": 9,\n",
        "        \"CWE-416\": 10\n",
        "    }\n",
        "\n",
        "    })\n",
        "\n",
        "    # 설정된 디바이스와 GPU 정보 출력\n",
        "    logger.info(f\"Device: {args.device}, Number of GPUs: {args.n_gpu}\")\n",
        "\n",
        "    # Random seed 설정\n",
        "    set_seed(args)\n",
        "\n",
        "    # Load tokenizers and models\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        num_labels=10  # CWE 유형의 수, num_cwe_types 대신 명시적으로 10 사용\n",
        "    )\n",
        "    model.to(args.device)\n",
        "\n",
        "    # 데이터셋 로드\n",
        "    train_dataset = TextDataset(tokenizer, args, cwe_label_map, file_type='train')\n",
        "    eval_dataset = TextDataset(tokenizer, args, cwe_label_map, file_type='eval')\n",
        "    test_dataset = TextDataset(tokenizer, args, cwe_label_map, file_type='test')\n",
        "\n",
        "\n",
        "    # 훈련과 평가 실행\n",
        "    if args.do_train:\n",
        "        logger.info(\"Starting training...\")\n",
        "        train(args, model, train_dataset, eval_dataset, args.cwe_label_map)\n",
        "\n",
        "    if args.do_eval:\n",
        "        logger.info(\"Starting evaluation...\")\n",
        "        evaluate(args, model, eval_dataset)\n",
        "\n",
        "    if args.do_test:\n",
        "        logger.info(\"Starting testing...\")\n",
        "        test(args, model, tokenizer, test_dataset)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "UCCkuq-BFReg",
        "outputId": "023ad418-5526-4864-fff5-45c8c6876a38"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-116-533f6d5cf39f>:8: DtypeWarning: Columns (107) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "  0%|          | 0/26228 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 26228/26228 [01:34<00:00, 278.36it/s]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 480/480 [00:01<00:00, 269.10it/s]\n",
            "  0%|          | 0/32605 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 32605/32605 [02:22<00:00, 228.34it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'RobertaForSequenceClassification' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-d86f9728323f>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-d86f9728323f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwe_label_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-35e2876fead0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer, eval_dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcwe_label_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwe_label_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 데이터 로더 구성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"replacement should be a boolean value, but got replacement={self.replacement}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36mnum_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# dataset size might change at runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'RobertaForSequenceClassification' has no len()"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}